---
title: Run Python scripts through Data Factory - Azure Batch Python 
description: Tutorial - Learn how to run Python scripts as part of a pipeline through Azure Data Factory using Azure Batch.
services: batch
author: mammask
manager: jeconnoc

ms.service: batch
ms.devlang: python
ms.topic: tutorial
ms.date: 12/11/2019
ms.author: komammas
ms.custom: mvc
---

# Tutorial: Run Python scripts through Azure Data Factory using Azure Batch

Run your Python workloads as part of an Azure Data Factory pipeline. The example below runs a script in Python that receives inputs (iris.csv) from an Azure Blob Storage account, performs a data manipulation process, and returns the outputs back to the Azure Blob Storage account. You learn how to:

> [!div class="checklist"]
> * Authenticate with Batch and Storage accounts
> * Develop and run a script in Python
> * Create a pool of compute nodes to run an application
> * Schedule your Python workloads
> * Monitor your analytics pipeline
> * Access your logfiles

If you donâ€™t have an Azure subscription, create a [free account](https://azure.microsoft.com/free/?WT.mc_id=A261C142F) before you begin.

## Prerequisites

* An installed [Python](https://www.python.org/downloads/) distribution

* [Azure library](https://pypi.org/project/azure/) for Python

* An Azure Batch account, an Azure Storage account, and an Azure Data Factory account. To create these accounts, see the Batch quickstarts using the [Azure portal](quick-create-portal.md) or [Azure CLI](quick-create-cli.md).

## Sign in to Azure

Sign in to the Azure portal at [https://portal.azure.com](https://portal.azure.com).

[!INCLUDE [batch-common-credentials](../../includes/batch-common-credentials.md)]

## Set up an Azure Storage Account

Set up your Azure Storage account using the [Azure portal](https://docs.microsoft.com/en-us/azure/storage/common/storage-quickstart-create-account?tabs=azure-portal). In the following example, you will access your **Azure Blob Storage** container and will upload the **iris.csv** file.

1.  Select **Overview** &gt; **Blobs**

    ![](https://github.com/mammask/tutorial-run-r-batch-azure-data-factory/blob/master/samples/setup-azure-blob-storage.png?raw=true)

2.  Select your storage container (e.g. **mystorage**)

    ![](https://raw.githubusercontent.com/mammask/tutorial-run-r-batch-azure-data-factory/master/samples/select-blob-storage-container.PNG)

3.  **Upload** &gt; **Files** &gt; [iris.csv](https://github.com/mammask/azure-docs-pr/blob/master/articles/batch/media/run-python-batch-azure-data-factory/iris.csv) &gt; **Upload to folder** &gt; inputs. A new folder will be created even if it does not exist.

    ![](https://raw.githubusercontent.com/mammask/tutorial-run-r-batch-azure-data-factory/master/samples/upload-to-blob-storage-container.PNG)

When the dataset is uploaded, the status of the current uploads will be updated to completed.

## Run the Python script

The prerequisite packages along with the Python script are described in the following sections.

### Install Azure library for `Python`

```bash
pip install azure
```

### Develop a script in `Python`

The following script in `Python` loads a dataset from **Azure Blob Storage** &gt; mystorage. It performs a data manipulation process and saves the results back to the Azure Blob Storage container in a different folder.

``` python
# Load libraries
from azure.storage.blob import BlockBlobService
import subprocess
import sys

# Install pandas
subprocess.check_call([sys.executable, "-m", "pip", "install", "pandas"])
import pandas as pd

# Define parameters
storageAccountName = "<storage-account-name>"
storageKey         = "<storage-account-key>"
containerName      = "<container-name>"

# Establish connection with the blob storage account
blobService = BlockBlobService(account_name=storageAccountName,
                               account_key=storageKey
                               )

# Load iris dataset from the task node
df = pd.read_csv("iris.csv")

# Subset records
df = df[df['Species'] == "setosa"]

# Save the subset of the iris dataframe locally in task node
df.to_csv("iris_setosa.csv", index = False)

# Upload iris dataset
blobService.create_blob_from_text(containerName, "outputs/iris_setosa.csv", "iris_setosa.csv")
```

Save the script as *main.python* and upload it to the **Azure Storage** container.

![Upload Blob](https://raw.githubusercontent.com/mammask/azure-docs-pr/master/articles/batch/media/run-python-batch-azure-data-factory/upload_script_blob.PNG?token=ABTT2GLMT3Y7CPENRHECGPS57NLTG)

Run the script locally using either command prompt or terminal depending on your operating system.

``` bash
python main.py
```

Access the **Azure portal** and check that the dataset is now saved in the outputs folder of the **Azure Storage** container.

![Upload Subset](https://raw.githubusercontent.com/mammask/azure-docs-pr/master/articles/batch/media/run-python-batch-azure-data-factory/upload_subset_data_blob.PNG?token=ABTT2GP4JLQ4PVU3RWSLKKC57NLVG)


### Set up an Azure Batch account

Set up your **Azure Batch** account using the [Azure portal](%22https://docs.microsoft.com/en-us/azure/batch/quick-create-portal%22).

1.  Select **Create a resource** &gt; **Compute** &gt; **Batch Service**. We named the Batch service as **mybatchtest**.

2.  Enter values for **Account name** and **Resource group**. The account name must be unique within the Azure Location selected, use only lowercase characters or numbers, and contain 3-24 characters.

3.  In **Storage account**, select an existing storage account (the one created in the previous step).

4.  Keep the defaults for the remaining settings, and select **Create** to create the account.

For more information about how to setup an Azure Batch account you can follow the official [documentation](https://docs.microsoft.com/en-us/azure/batch/batch-account-create-portal).


### Create a pool of compute nodes

Now that the Batch account is set up and is associated with the Blob storage account, create a set of pool nodes to run the `Python` script.

1.  Access the Batch account and select **Pools**

2.  Select **Add** to add a new Pool

    ![](https://github.com/mammask/tutorial-run-r-batch-azure-data-factory/blob/master/samples/create-pool.PNG?raw=true)

3.  Set a Pool ID

4.  Enter **MarketPlace (Linux/Windows)** as an Image Type

5.  Enter **microsoft-dsvm** as a Publisher

6.  Enter a **dsvm-windows** as an offer

7.  Select a very small VM for this exmaple: **Standard A1 (1 Cores, 1.8GB Memory)**

8.  Pick one **Target dedicated node**

9.  Keep defaults for the remaining settings, and select **OK** to create the pool


### Set up Azure Data Factory pipeline

In this procedure, you create and validate a pipeline using a script in `Python` to load a .csv dataset from a **Blob Storage** container, perform a data manipulation process, and output the results of the analysis to a different folder in the **Blob Storage** container. For more information about how to create pipelines using **Data Factory** you can follow the official [documentation](https://docs.microsoft.com/en-us/azure/data-factory/concepts-pipelines-activities).

1.  In the **Factory Resources** box, select the + (plus) button and then select **Pipeline**

2.  In the **General** tab specify **run Python** for Name

    ![](https://raw.githubusercontent.com/mammask/azure-docs-pr/master/articles/batch/media/run-python-batch-azure-data-factory/adf-create-pipeline.png?token=ABTT2GMIRJJPYBVS7ADEC2S5ZLZL6)

3.  In the **Activities** box, expand the **Batch Service**. Drag the **Custom** activity from the **Activities** toolbox to the pipeline designer surface.

4.  In the **General** tab specify **testPipeline** for Name

    ![](https://raw.githubusercontent.com/mammask/azure-docs-pr/master/articles/batch/media/run-python-batch-azure-data-factory/adf-create-custom-task.png?token=ABTT2GPZKBE6AN74MJ3QBG25ZLZQW)


5.  In the **Azure Batch** tab, add the **Batch Account** that was created in the previous steps and **Test connection** to ensure that it is successful

    ![](https://github.com/mammask/tutorial-run-r-batch-azure-data-factory/blob/master/samples/integrate-pipeline-with-azure-batch.PNG?raw=true)

6.  In the **Settings** tab, enter the command `python main.py`

7.  Add the **Batch Account** that was created in the previous steps in the **Resource Linked Service** and **Test connection** to ensure that it is successful

8.  In the **Folder Path**, select the name of the **Azure Blob Storage** container that contains the `Python` script and the associated inputs. This will download the selected files from the container to the pool node instances before the execution of the `Python` script.

    ![](https://raw.githubusercontent.com/mammask/azure-docs-pr/master/articles/batch/media/run-python-batch-azure-data-factory/adf-create-custom-task-python-script-command.PNG?token=ABTT2GLQOQ4OPMOW6WFMUT25ZLZTI)

9.  Click **Validate** on the pipeline toolbar above the canvas to validate the pipeline settings. Confirm that the pipeline has been successfully validated. To close the validation output, select the &gt;&gt; (right arrow) button.

10. Click **debug** to debug the pipeline

11. Click **publish** to publish the pipeline

12. Click **trigger** to run the `Python` script as part of a batch process

    ![](https://raw.githubusercontent.com/mammask/azure-docs-pr/master/articles/batch/media/run-python-batch-azure-data-factory/adf-create-custom-task-python-success-run.PNG?token=ABTT2GKSGTINC5OX3VEXPXS5ZLZXK)

You can debug, trigger, monitor the pipeline manually, and schedule periodical executions of the script. For more information visit the Azure official [documentation](https://docs.microsoft.com/en-us/azure/batch/batch-account-create-portal).

### Monitor the log files

Access the log files of the batch process to view bugs/warnings produced by the execution of the `Python` script on the pool nodes. If the Azure Batch Custom Activity returns an error, then you can access the working directory of the pool node.

1.  Access your **Azure Batch** account

2.  Select **Jobs**

    ![](https://github.com/mammask/tutorial-run-r-batch-azure-data-factory/blob/master/samples/access-batch-jobs-log.PNG?raw=true)

3.  Select the pool id that is associated with the tasks that ran on **Azure Data Factory**

    ![](https://raw.githubusercontent.com/mammask/azure-docs-pr/master/articles/batch/media/run-python-batch-azure-data-factory/batch-jobs.PNG?token=ABTT2GIIHFIO53JY5WXSXWK57KZNG)

4.  Select the latest task

    ![](https://raw.githubusercontent.com/mammask/azure-docs-pr/master/articles/batch/media/run-python-batch-azure-data-factory/batch-jobs-successful-task.PNG?token=ABTT2GJNVYPK6WLJULQQ4FS57KZ22)

5.  The **wd** folder is the working directory of the pool node

    ![](https://raw.githubusercontent.com/mammask/azure-docs-pr/master/articles/batch/media/run-python-batch-azure-data-factory/batch-jobs-successful-task-wd.PNG?token=ABTT2GLQAGX5T2DUZLGMSNC57KZ54)

6.  The **stder.txt** file is the logs file associated with the execution of the `Python` script

    ![](https://raw.githubusercontent.com/mammask/azure-docs-pr/master/articles/batch/media/run-python-batch-azure-data-factory/batch-jobs-successful-task-wd-stderr.PNG?token=ABTT2GP2JVBXJKUMPPEDASK57KZ72)

In this example, there is no issue.

## Clean up resources

You are charged for compute nodes, so we recommend that you allocate pools only as needed. When you delete the pool, all task output on the nodes is deleted. However, the input and output files remain in the storage account.

When no longer needed, delete the resource group, Batch account, and storage account. To do so in the Azure portal, select the resource group for the Batch account and click **Delete resource group**.

## Next steps

In this tutorial, you explored an example that taught you how to run Python scripts as part of a pipeline through Azure Data Factory using Azure Batch.

To learn more about Azure Data Factory, see:

> [!div class="nextstepaction"]
> [Azure Data Factory](../data-factory/introduction.md)
