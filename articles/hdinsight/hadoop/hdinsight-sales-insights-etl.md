---
title: Create end to end ETL pipelines to derive sales insights
description: Learn how to use create ETL pipelines with Azure HDInsight to derive insights from sales data using Spark on-demand clusters and Power BI.
keywords: hdinsight,hadoop,hive,interactive query,interactive hive,LLAP,odbc 
author: hrasheed-msft
ms.reviewer: jasonh
ms.service: hdinsight
ms.custom: hdinsightactive
ms.topic: tutorial
ms.date: 08/05/2019
ms.author: hrasheed
---
# End to End ETL Pipeline using Azure HDInsight

You are a large company with stores all across the US that sells many products in different departments. You have large amounts of sales data and would like to see business insights from it. After looking into Microsoft’s Azure HDInsight you think this might be a great tool to understanding data associated with the company.

You decide to build an Extract, Transform and Load pipeline using Azure services. This will allow you to combine the data from all your different stores, remove any unwanted data, append new data, and load this back to your storage in order to visualize business insights. Read more about ETL pipelines [here](https://docs.microsoft.com/en-us/azure/hdinsight/hadoop/apache-hadoop-etl-at-scale).

You also see that you are able to build out this entire pipeline using Azure products, with HDInsight being the primary mode for data transformations – given the large volume of data associated with the multitudes of stores, a big data product like HDInsight provides the perfect solution to transforming and analyzing this data quickly and at scale. 

INSERT IMAGE OF ARCHITECTURE HERE 

![This is my alt-text](./media/hdinsight-sales-insights-etl/Untitled.png)

### Azure Subscription
If you don't have an Azure subscription, create a [free account](https://azure.microsoft.com/free/) before you begin.

### Azure Storage Explorer 
Download [Azure Storage Explorer]() to easily browse storage accounts.
Download [Power BI Desktop]() to visualize business insights at the end of this tutorial. 

### Sign into Azure

On the Azure CLI log into your Azure account and set the subscription. Set up the resource group for the project. 

### Set relevant variables for easy use
```azurecli-interactive 
$rg = "<RESOURCE GROUP NAME>"
```

```azurecli-interactive 
az login
az account set --subscription <SUBSCRIPTION_ID> 
az group create --name $rg --location westus
```
### Download relevant files for this project

Clone this [github repository]() on your local machine. 

- `/salesdata/`
- `clusterstemplate.json`, `clustersparameters.json`
- `adftemplate.json`, `adfparameters.json`
- `sparktransform.py` 

Navigate to this directory on your Azure CLI. 


## Deploy Azure resources needed for the pipeline 

This section will deploy the following resources: 
1. Azure Blob Storage - to mimic your company's storage of raw data
2. ADLS Gen2 Storage - the storage account for both HDInsight clusters
3. Managed Identity - to give clusters access to the storage account
4. Spark Cluster - to clean up and transform the raw data
5. Interactive Query Cluster - to allow quick quering and data visualization on Power BI
6. VNET supported by NSG rules - to provide security to your clusters 

### Deploy the ARM template

The `clusterstemplate.json` ARM template configures all the above resources. The default password used for ssh access to the clusters is `Thisisapassword1`. To change the password navigate to `clustersparameters.json` file and change the password. 

```azurecli-interactive 
az group deployment create --name ResourcesDeployment \
    --resource-group $resourceGroup \
    --template-file clusterstemplate.json \
    --parameters clustersparameters.json
```
Note: Cluster creation can take around 20 minutes. 
#### Upload the salesdata files by executing the command 
```
az storage blob upload-batch -d rawdata --account-name $blobStorageName -s directory_path --pattern *.csv
```

#### Sanity check: 
If you want to make sure this step worked correctly, go to the resource group on the azure portal and check all the outlined resources were deployed. You can also check that the data is uploaded to the Blob Storage account. 

## Create an Azure Data Factory

Azure Data Factory is a tool that helps automate Azure pipelines. It is not the only way to accomplish these tasks, but it's a great way to automate these processes. Read more about it [here](https://azure.microsoft.com/en-us/services/data-factory/). 

This Azure Data Factory will have 2 pipelines: 

1. The first pipeline will copy the data from the Azure Blob Storage to the ADLS Gen 2 Storage Account to mimic data ingestion. 
2. The second pipeline will run the spark script on the Spark cluster to transform the data by cleaning up and removing unwanted columns as well as creating a new column that calculates the revenue generated by a single transaction.
   
Create a FileSystem on the ADLS Gen2 storage account called `files`. This is where your data will be stored. Within `files`, make 3 folders named `data`, `transformed`, and `adf`. These will contain the raw data, the transformed data, and the adf scripts, respectively. Within the adf folder, the folder structure needs to look like it is outlined [here](https://docs.microsoft.com/en-us/azure/data-factory/transform-data-using-spark). This is important because the ADF pipeline will not succeed unless the folder structure is correct. (Looking into ways to do this from the command line)

Additionally, fill out all needed variables in the `sparktransform.py` file and `adfparameters.json`. Connection strings/account keys of storage accounts can be found by navigating to them on the portal and selecting Account keys. You can also find them using the command
```
az storage account keys list \
    --account-name storageName \
    --resource-group myResourceGroup \
    --output table
```
Deploy the ADF using the following command: 

```azurecli-interactive 
az group deployment create --name ADFDeployment \
    --resource-group $resourceGroup \
    --template-file adf.json \
    --parameters "@${adfparameters.json}"
```

### Trigger the Pipelines
Run the following commands to trigger the ADF pipelines

This pipeline moves the data from blob storage to ADLS Gen2 Storage
```powershell
Invoke-AzDataFactoryV2Pipeline -DataFactory $df -PipelineName "CopyPipeline_k8z" 
```
This pipeline applies the spark transformations on the data
```powershell
Invoke-AzDataFactoryV2Pipeline -DataFactory $df -PipelineName "SparkTransform"

```

## Create a table on the Interactive Query cluster to view data on Power BI

Navigate to the Interactive Query Cluster on the Azure portal and open up the interactive query editor. Login using the credentials you set for the Interactive Query cluster. If you did not change any parameters this should be `username=admin` and `password=Thisisapassword1`. Add in your ADLS Gen2 Storage account name where it is indicated and then  execute: 

```
DROP TABLE sales_raw;
-- Creates an external table over the csv file
CREATE EXTERNAL TABLE sales_raw(
  REGION STRING,
  STORE STRING,
  SALEDATE STRING,
  DEP STRING,
  ITEM STRING,
  UNITSOLD INT,
  UNITPRICE INT,
  REVENUE INT,
  CUSTOMERID INT, 
  LOYALTY BOOLEAN, 
  FIRSTPURCHASE STRING,
  FREQ INT)
--Format and location of the file
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
STORED AS TEXTFILE
LOCATION 'abfs://files@<ADLS STORAGE NAME>.dfs.core.windows.net/transformed';
--Drop table sales if exists
DROP TABLE sales;
--Create sales table and populate with data\
--pulled in from csv file (via external table defined previously)
CREATE TABLE sales AS
SELECT REGION AS region,
	STORE AS store,
	CAST(SALEDATE AS DATE) as saledate, 
	DEP as dep,
	ITEM as item,
	UNITSOLD as unitsold,
	UNITPRICE as unitprice,
	REVENUE as revenue,
	CUSTOMERID as customerID,
	LOYALTY as loyalty,
	FIRSTPURCHASE as firstpurchase,
	FREQ as freq
FROM sales_raw;

```

This will create a Table on the Interactive Query cluster that you can access from Power BI. Open up Power BI Desktop and select Get Data. Search for HDInsight Interactive Query cluster and paste the URI for your cluster there. Type `default` for the database. 

Once the data is loaded, you can experiment with the dashboard you would like to create. Here is an example dashboard with the given data. 

## Clean up resources

If you're not going to continue to use this application, delete
all resources with the following steps so that you are not charged for them. 

```azurecli-interactive 
az group delete -n $resourceGroup
```

## Next steps

Advance to the next article to learn how to create...
> [!div class="nextstepaction"]
> [Next steps button](contribute-get-started-mvc.md)

<!--- Required:
Tutorials should always have a Next steps H2 that points to the next
logical tutorial in a series, or, if there are no other tutorials, to
some other cool thing the customer can do. A single link in the blue box
format should direct the customer to the next article - and you can
shorten the title in the boxes if the original one doesn’t fit.
Do not use a "More info section" or a "Resources section" or a "See also
section". --->