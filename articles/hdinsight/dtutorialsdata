hdinsight-administer-use-portal-linux.md:    * **Data Lake Store access**: Configure access Data Lake Stores.  See [Create HDInsight clusters with Data Lake Store by using the Azure portal](../data-lake-store/data-lake-store-hdinsight-hadoop-use-portal.md).
hdinsight-administer-use-portal-linux.md:You can add additional Azure Storage accounts and Azure Data Lake Store accounts after a cluster is created. For more information, see [Add additional storage accounts to HDInsight](./hdinsight-hadoop-add-storage.md).
hdinsight-administer-use-portal-linux.md:HDInsight clusters use either an Azure Storage account or an Azure Data Lake Store to store data. Each HDInsight cluster can have one default storage account and a number of linked storage accounts. To list the storage accounts, you first open the cluster from the portal, and then click **Storage accounts**:
hdinsight-administer-use-portal-linux.md:To list the Data Lake Store accounts, click **Data Lake Store access** in the previous screenshot.
hdinsight-administer-use-powershell.md:$defaultStoreageType = $storageInfo[1]
hdinsight-administer-use-powershell.md:echo "Default Storage account type: $defaultStoreageType"
hdinsight-administer-use-powershell.md:if ($defaultStoreageType -eq "blob")
hdinsight-analyze-flight-delay-data.md:1. **Store data in Azure Blob storage.**  For example, weather data, sensor data, web logs, and in this case, flight delay data are saved into Azure Blob storage.
hdinsight-apache-storm-with-kafka.md:* `${hdfs.url}`: The file system URL for the HDFSBolt component. Indicates whether the data is written to an Azure Storage account or Azure Data Lake Store.
hdinsight-apache-storm-with-kafka.md:| `hdfs.url` | The file system used by the Storm cluster. For Azure Storage accounts, use a value of `wasb:///`. For Azure Data Lake Store, use a value of `adl:///`. |
hdinsight-apache-storm-with-kafka.md:    > The `hdfs.url` entry is configured for a cluster that uses an Azure Storage account. To use this topology with a Storm cluster that uses Data Lake Store, change this value from `wasb` to `adl`.
hdinsight-capacity-planning.md:The default storage, either an Azure Storage account or Azure Data Lake Store, must be in the same location as your cluster. Azure Storage is available at all locations. Data Lake Store is available in some regions - see the current Data Lake Store availability under *Storage* in [Azure Products Available by Region](https://azure.microsoft.com/regions/services/).
hdinsight-capacity-planning.md:If you already have a storage account or Data Lake Store containing your data and want to use this storage as your cluster's default storage, then you must deploy your cluster at that same location.
hdinsight-capacity-planning.md:After you have an HDInsight cluster deployed, you can attach additional Azure Storage accounts or access other Data Lake Stores. All your storage accounts must reside in the same location as your cluster. A Data Lake Store can be in a different location, although this may introduce some data read/write latency.
hdinsight-capacity-planning.md:Azure Storage has some [capacity limits](../azure-subscription-service-limits.md#storage-limits), while  Data Lake Store is virtually unlimited.
hdinsight-capacity-planning.md:* Network: For most cluster types, the data processed by the cluster is not on local disk, but rather in an external storage service such as Data Lake Store or Azure Storage. Consider the network bandwidth and throughput between the node VM and the storage service. The network bandwidth available to a VM typically increases with larger sizes. For details, see [VM sizes overview](https://docs.microsoft.com/azure/virtual-machines/linux/sizes).
hdinsight-component-versioning.md:### Support for Azure Data Lake Store
hdinsight-component-versioning.md:The Enterprise Security Package supports using Azure Data Lake Store as both the primary storage and the add-on storage.
hdinsight-component-versioning.md:Starting with HDInsight version 3.4, Microsoft has released HDInsight only on the Linux OS. As a result, some of the components within HDInsight are available for Linux only. These include Apache Ranger, Kafka, Interactive Query, Spark, HDInsight applications, and Azure Data Lake Store as the primary file system. Future releases of HDInsight are available only on the Linux OS. There will be no future releases of HDInsight on Windows. 
hdinsight-component-versioning.md:Starting with HDInsight version 3.4, Microsoft has released HDInsight only on the Linux OS. As a result, some of the components within HDInsight are available for Linux only. These include Apache Ranger, Kafka, Interactive Query, Spark, HDInsight applications, and Azure Data Lake Store as the primary file system. 
hdinsight-connect-hive-zeppelin.md:* [Connect to Azure HDInsight and run Hive queries using Data Lake Tools for Visual Studio](hadoop/apache-hadoop-visual-studio-tools-get-started.md).
hdinsight-delete-cluster.md:> Deleting an HDInsight cluster does not delete the Azure Storage accounts or Data Lake Store associated with the cluster. You can reuse data stored in those services in the future.
hdinsight-hadoop-architecture.md:> An HDFS is not typically deployed within the HDInsight cluster to provide storage. Instead, an HDFS-compatible interface layer is used by Hadoop  components. The actual storage capability is provided by either Azure Storage or Azure Data Lake Store. For Hadoop, MapReduce jobs executing on the HDInsight cluster run as if an HDFS were present and so require no changes to support their storage needs. In Hadoop on HDInsight, storage is outsourced, but YARN processing  remains a core component. For more information, see [Introduction to Azure HDInsight](hadoop/apache-hadoop-introduction.md).
hdinsight-hadoop-create-linux-clusters-portal.md:4. For **Storage**, specify whether you want Azure Storage (WASB) or Data Lake Store as your default storage. Look at the table below for more information.
hdinsight-hadoop-create-linux-clusters-portal.md:	| **Azure Storage Blobs as default storage**   | <ul><li>For **Primary Storage type**, select **Azure Storage**. After that, for **Selection method**, you can choose **My subscriptions** if you want to specify a storage account that is part of your Azure subscription and then select the storage account. Otherwise, click **Access key** and provide the information for the storage account that you want to choose from outside your Azure subscription.</li><li>For **Default container**, you can choose to go with the default container name suggested by the portal or specify your own.</li><li>If you are using WASB as default storage, you can (optionally) click **Additional Storage Accounts** to specify additional storage accounts to associate with the cluster. For **Azure Storage Keys**, click **Add a storage key**, and then you can provide a storage account from your Azure subscriptions or from other subscriptions (by providing the storage account access key).</li><li>If you are using WASB as default storage, you can (optionally) click **Data Lake Store access** to specify Azure Data Lake Store as additional storage. For more information, see [Create an HDInsight cluster with Data Lake Store using Azure portal](../data-lake-store/data-lake-store-hdinsight-hadoop-use-portal.md).</li></ul> |
hdinsight-hadoop-create-linux-clusters-portal.md:	| **Azure Data Lake Store as default storage** | For **Primary storage type**, select **Data Lake Store** and then refer to the article [Create an HDInsight cluster with Data Lake Store using Azure portal](../data-lake-store/data-lake-store-hdinsight-hadoop-use-portal.md) for instructions. |
hdinsight-hadoop-customize-cluster-linux.md:    * An **Azure Data Lake Store** account that is accessible by the HDInsight cluster. For information on using Azure Data Lake Store with HDInsight, see [Create an HDInsight cluster with Data Lake Store](../data-lake-store/data-lake-store-hdinsight-hadoop-use-portal.md).
hdinsight-hadoop-customize-cluster-linux.md:        When using a script stored in Data Lake Store, the URI format is `adl://DATALAKESTOREACCOUNTNAME.azuredatalakestore.net/path_to_file`.
hdinsight-hadoop-customize-cluster-linux.md:        > The service principal HDInsight uses to access Data Lake Store must have read access to the script.
hdinsight-hadoop-development-using-azure-resource-manager.md:* **Add-AzureRmHDInsightClusterIdentity**: Adds a cluster identity to a cluster configuration object so that the HDInsight cluster can access Azure Data Lake Stores. See [Create an HDInsight cluster with Data Lake Store using Azure PowerShell](../data-lake-store/data-lake-store-hdinsight-hadoop-use-powershell.md).
hdinsight-hadoop-emulator-visual-studio.md:title: Data Lake tools for Visual Studio with Hortonworks Sandbox - Azure HDInsight | Microsoft Docs
hdinsight-hadoop-emulator-visual-studio.md:description: Learn how to use the Azure Data Lake tools for Visual Studio with the Hortonworks sandbox running in a local VM. With these tools, you can create and run Hive and Pig jobs on the sandbox, and view job output and history.
hdinsight-hadoop-emulator-visual-studio.md:# Use the Azure Data Lake tools for Visual Studio with the Hortonworks Sandbox
hdinsight-hadoop-emulator-visual-studio.md:Azure Data Lake includes tools for working with generic Hadoop clusters. This document provides the steps needed to use the Data Lake tools with the Hortonworks Sandbox running in a local virtual machine.
hdinsight-hadoop-emulator-visual-studio.md:* The [Azure Data Lake tools for Visual Studio](https://www.microsoft.com/download/details.aspx?id=49504).
hdinsight-hadoop-emulator-visual-studio.md:    > The update process uses Ambari to modify the Hortonworks Sandbox configuration to what is expected by the Data Lake tools for Visual Studio.
hdinsight-hadoop-emulator-visual-studio.md:2. From the list of projects, expand **Templates**, expand **Azure Data Lake**, and then select **HIVE (HDInsight)**. From the list of templates, select **Hive Sample**. Enter a name and location, and then select **OK**.
hdinsight-hadoop-emulator-visual-studio.md:    ![Screenshot of New Project window, with Azure Data Lake, HIVE, Hive Sample, and OK highlighted](./media/hdinsight-hadoop-emulator-visual-studio/new-hive-project.png)
hdinsight-hadoop-emulator-visual-studio.md:1. Open Visual Studio, and select **File**, **New**, and then **Project**. From the list of projects, expand **Templates**, expand **Azure Data Lake**, and then select **Pig (HDInsight)**. From the list of templates, select **Pig Application**. Enter a name, location, and then select **OK**.
hdinsight-hadoop-emulator-visual-studio.md:    ![Screenshot of New Project window, with Azure Data Lake, Pig, Pig Application, and OK highlighted](./media/hdinsight-hadoop-emulator-visual-studio/new-pig.png)
hdinsight-hadoop-emulator-visual-studio.md:Data Lake tools also allow you to easily view information about jobs that have been run on Hadoop. Use the following steps to see the jobs that have been run on the local cluster.
hdinsight-hadoop-hue-linux.md:The script to install Hue on a Linux-based HDInsight cluster is available at https://hdiconfigactions.blob.core.windows.net/linuxhueconfigactionv02/install-hue-uber-v02.sh. You can use this script to install Hue on clusters with either Azure Storage Blobs (WASB) or Azure Data Lake Store as default storage.
hdinsight-hadoop-install-presto.md:	* It must use Azure Storage as the data store. Using Presto on a cluster that uses Azure Data Lake Store as the storage option is not yet supported. 
hdinsight-hadoop-linux-information.md:## HDFS, Azure Storage, and Data Lake Store
hdinsight-hadoop-linux-information.md:HDInsight uses either blobs in Azure Storage or Azure Data Lake Store as the default store. These services provide the following benefits:
hdinsight-hadoop-linux-information.md:An Azure Storage account can hold up to 4.75 TB, though individual blobs (or files from an HDInsight perspective) can only go up to 195 GB. Azure Data Lake Store can grow dynamically to hold trillions of files, with individual files greater than a petabyte. For more information, see [Understanding blobs](https://docs.microsoft.com/rest/api/storageservices/understanding-block-blobs--append-blobs--and-page-blobs) and [Data Lake Store](https://azure.microsoft.com/services/data-lake-store/).
hdinsight-hadoop-linux-information.md:When using either Azure Storage or Data Lake Store, you don't have to do anything special from HDInsight to access the data. For example, the following command lists files in the `/example/data` folder regardless of whether it is stored on Azure Storage or Data Lake Store:
hdinsight-hadoop-linux-information.md:When using __Data Lake Store__, use one of the following URI schemes:
hdinsight-hadoop-linux-information.md:* `adl:///`: Access the default Data Lake Store for the cluster.
hdinsight-hadoop-linux-information.md:* `adl://<storage-name>.azuredatalakestore.net/`: Used when communicating with a non-default Data Lake Store. Also used to access data outside the root directory of your HDInsight cluster.
hdinsight-hadoop-linux-information.md:> When using Data Lake Store as the default store for HDInsight, you must specify a path within the store to use as the root of HDInsight storage. The default path is `/clusters/<cluster-name>/`.
hdinsight-hadoop-linux-information.md:* `adl://home` if using Azure Data Lake Store. To get the Data Lake Store name, use the following REST call:
hdinsight-hadoop-linux-information.md:If using __Azure Data Lake Store__, see the following links for ways that you can access your data:
hdinsight-hadoop-linux-information.md:* [Data Lake Tools for Visual Studio](https://www.microsoft.com/download/details.aspx?id=49504)
hdinsight-hadoop-linux-use-ssh-unix.md:> * [HDInsight using Azure Data Lake Store](hdinsight-hadoop-use-data-lake-store.md).
hdinsight-hadoop-manage-ambari-rest-api.md:When you create an HDInsight cluster, you must use an Azure Storage Account or Data Lake Store as the default storage for the cluster. You can use Ambari to retrieve this information after the cluster has been created. For example, if you want to read/write data to the container outside HDInsight.
hdinsight-hadoop-manage-ambari-rest-api.md:* `adl://home` - This value indicates that the cluster is using an Azure Data Lake Store for default storage.
hdinsight-hadoop-manage-ambari-rest-api.md:    To find the Data Lake Store account name, use the following examples:
hdinsight-hadoop-manage-ambari-rest-api.md:    The return value is similar to `ACCOUNTNAME.azuredatalakestore.net`, where `ACCOUNTNAME` is the name of the Data Lake Store account.
hdinsight-hadoop-manage-ambari-rest-api.md:    To find the directory within Data Lake Store that contains the storage for the cluster, use the following examples:
hdinsight-hadoop-manage-ambari-rest-api.md:    The return value is similar to `/clusters/CLUSTERNAME/`. This value is a path within the Data Lake Store account. This path is the root of the HDFS compatible file system for the cluster. 
hdinsight-hadoop-optimize-hive-query.md:To enable ORC format, you first create a table with the clause *Stored as ORC*:
hdinsight-hadoop-provision-linux-clusters.md:Although an on-premises installation of Hadoop uses the Hadoop Distributed File System (HDFS) for storage on the cluster, in the cloud you use storage endpoints connected to cluster. HDInsight clusters use either [Azure Data Lake Store](hdinsight-hadoop-use-data-lake-store.md) or [blobs in Azure Storage](hdinsight-hadoop-use-blob-storage.md). Using Azure Storage or Data Lake Store means you can safely delete the HDInsight clusters used for computation while still retaining your data. 
hdinsight-hadoop-provision-linux-clusters.md:During configuration, for the default storage endpoint you specify a blob container of an Azure Storage account or a Data Lake Store. The default storage contains application and system logs. Optionally, you can specify additional linked Azure Storage accounts and Data Lake Store accounts that the cluster can access. The HDInsight cluster and the dependent storage accounts must be in the same Azure location.
hdinsight-hadoop-script-actions-linux.md:Components that you install on the cluster might have a default configuration that uses Hadoop Distributed File System (HDFS) storage. HDInsight uses either Azure Storage or Data Lake Store as the default storage. Both provide an HDFS compatible file system that persists data even if the cluster is deleted. You may need to configure components you install to use WASB or ADL instead of HDFS.
hdinsight-hadoop-script-actions-linux.md:In this example, the `hdfs` command transparently uses the default cluster storage. For some operations, you may need to specify the URI. For example, `adl:///example/jars` for Data Lake Store or `wasb:///example/jars` for Azure Storage.
hdinsight-hadoop-script-actions-linux.md:* An __Azure Data Lake Store account__ that is associated with the HDInsight cluster. For more information on using Azure Data Lake Store with HDInsight, see [Create an HDInsight cluster with Data Lake Store](../data-lake-store/data-lake-store-hdinsight-hadoop-use-portal.md).
hdinsight-hadoop-script-actions-linux.md:    > The service principal HDInsight uses to access Data Lake Store must have read access to the script.
hdinsight-hadoop-script-actions-linux.md:Storing the files in an Azure Storage account or Azure Data Lake Store provides fast access, as both within the Azure network.
hdinsight-hadoop-script-actions-linux.md:> The URI format used to reference the script differs depending on the service being used. For storage accounts associated with the HDInsight cluster, use `wasb://` or `wasbs://`. For publicly readable URIs, use `http://` or `https://`. For Data Lake Store, use `adl://`.
hdinsight-hadoop-templeton-webhcat-debug-errors.md:Learn about errors received when using WebHCat with HDInsight, and how to resolve them. WebHCat is used internally by client-side tools such as Azure PowerShell and the Data Lake Tools for Visual Studio.
hdinsight-hadoop-use-blob-storage.md:description: Learn how to query data from Azure storage and Azure Data Lake Store to store results of your analysis.
hdinsight-hadoop-use-blob-storage.md:To analyze data in HDInsight cluster, you can store the data either in Azure Storage, Azure Data Lake Store, or both. Both storage options enable you to safely delete HDInsight clusters that are used for computation without losing user data.
hdinsight-hadoop-use-blob-storage.md:Hadoop supports a notion of the default file system. The default file system implies a default scheme and authority. It can also be used to resolve relative paths. During the HDInsight cluster creation process, you can specify a blob container in Azure Storage as the default file system, or with HDInsight 3.5, you can select either Azure Storage or Azure Data Lake Store as the default files system with a few exceptions. For the supportability of using Data Lake Store as both the default and linked storage, see [Availabilities for HDInsight cluster](./hdinsight-hadoop-use-data-lake-store.md#availabilities-for-hdinsight-clusters).
hdinsight-hadoop-use-blob-storage.md:In this article, you learn how Azure Storage works with HDInsight clusters. To learn how Data Lake Store works with HDInsight clusters, see [Use Azure Data Lake Store with Azure HDInsight clusters](hdinsight-hadoop-use-data-lake-store.md). For more information about creating an HDInsight cluster, see [Create Hadoop clusters in HDInsight](hdinsight-hadoop-provision-linux-clusters.md).
hdinsight-hadoop-use-blob-storage.md:When creating an HDInsight cluster from the Portal, you have the options (as shown below) to provide the storage account details. You can also specify whether you want an additional storage account associated with the cluster, and if so, choose from Data Lake Store or another Azure Storage blob as the additional storage.
hdinsight-hadoop-use-blob-storage.md:* [Get started with Azure Data Lake Store](../data-lake-store/data-lake-store-get-started-portal.md)
hdinsight-hadoop-use-data-lake-store.md:title: Use Data Lake Store with Hadoop in Azure HDInsight | Microsoft Docs
hdinsight-hadoop-use-data-lake-store.md:description: Learn how to query data from Azure Data Lake Store and to store results of your analysis.
hdinsight-hadoop-use-data-lake-store.md:# Use Data Lake Store with Azure HDInsight clusters
hdinsight-hadoop-use-data-lake-store.md:To analyze data in HDInsight cluster, you can store the data either in [Azure Storage](../storage/common/storage-introduction.md), [Azure Data Lake Store](../data-lake-store/data-lake-store-overview.md), or both. Both storage options enable you to safely delete HDInsight clusters that are used for computation without losing user data.
hdinsight-hadoop-use-data-lake-store.md:In this article, you learn how Data Lake Store works with HDInsight clusters. To learn how Azure Storage works with HDInsight clusters, see [Use Azure Storage with Azure HDInsight clusters](hdinsight-hadoop-use-blob-storage.md). For more information about creating an HDInsight cluster, see [Create Hadoop clusters in HDInsight](hdinsight-hadoop-provision-linux-clusters.md).
hdinsight-hadoop-use-data-lake-store.md:> Data Lake Store is always accessed through a secure channel, so there is no `adls` filesystem scheme name. You always use `adl`.
hdinsight-hadoop-use-data-lake-store.md:Hadoop supports a notion of the default file system. The default file system implies a default scheme and authority. It can also be used to resolve relative paths. During the HDInsight cluster creation process, you can specify a blob container in Azure Storage as the default file system, or with HDInsight 3.5 and newer versions, you can select either Azure Storage or Azure Data Lake Store as the default files system with a few exceptions. 
hdinsight-hadoop-use-data-lake-store.md:HDInsight clusters can use Data Lake Store in two ways:
hdinsight-hadoop-use-data-lake-store.md:As of now, only some of the HDInsight cluster types/versions support using Data Lake Store as default storage and additional storage accounts:
hdinsight-hadoop-use-data-lake-store.md:| HDInsight cluster type | Data Lake Store as default storage | Data Lake Store as additional storage| Notes |
hdinsight-hadoop-use-data-lake-store.md:| Storm | | |You can use Data Lake Store to write data from a Storm topology. You can also use Data Lake Store for reference data that can then be read by a Storm topology.|
hdinsight-hadoop-use-data-lake-store.md:Using Data Lake Store as an additional storage account does not affect performance or the ability to read or write to Azure storage from the cluster.
hdinsight-hadoop-use-data-lake-store.md:## Use Data Lake Store as default storage
hdinsight-hadoop-use-data-lake-store.md:When HDInsight is deployed with Data Lake Store as default storage, the cluster-related files are stored in Data Lake Store in the following location:
hdinsight-hadoop-use-data-lake-store.md:where `<cluster_root_path>` is the name of a folder you create in Data Lake Store. By specifying a root path for each cluster, you can use the same Data Lake Store account for more than one cluster. So, you can have a setup where:
hdinsight-hadoop-use-data-lake-store.md:Notice that both the clusters use the same Data Lake Store account **mydatalakestore**. Each cluster has access to its own root filesystem in Data Lake Store. The Azure portal deployment experience in particular prompts you to use a folder name such as **/clusters/\<clustername>** for the root path.
hdinsight-hadoop-use-data-lake-store.md:To be able to use a Data Lake Store as default storage, you must grant the service principal access to the following paths:
hdinsight-hadoop-use-data-lake-store.md:- The Data Lake Store account root.  For example: adl://mydatalakestore/.
hdinsight-hadoop-use-data-lake-store.md:For more information for creating service principal and grant access, see [Configure Data Lake store access](#configure-data-lake-store-access).
hdinsight-hadoop-use-data-lake-store.md:## Use Data Lake Store as additional storage
hdinsight-hadoop-use-data-lake-store.md:You can use Data Lake Store as additional storage for the cluster as well. In such cases, the cluster default storage can either be an Azure Storage Blob or a Data Lake Store account. If you are running HDInsight jobs against the data stored in Data Lake Store as additional storage, you must use the fully-qualified path to the files. For example:
hdinsight-hadoop-use-data-lake-store.md:Note that there's no **cluster_root_path** in the URL now. That's because Data Lake Store is not a default storage in this case so all you need to do is provide the path to the files.
hdinsight-hadoop-use-data-lake-store.md:To be able to use a Data Lake Store as additional storage, you only need to grant the service principal access to the paths where your files are stored.  For example:
hdinsight-hadoop-use-data-lake-store.md:For more information for creating service principal and grant access, see [Configure Data Lake store access](#configure-data-lake-store-access).
hdinsight-hadoop-use-data-lake-store.md:## Use more than one Data Lake Store accounts
hdinsight-hadoop-use-data-lake-store.md:Adding a Data Lake Store account as additional and adding more than one Data Lake Store accounts are accomplished by giving the HDInsight cluster permission on data in one ore more Data Lake Store accounts. See [Configure Data Lake Store access](#configure-data-lake-store-access).
hdinsight-hadoop-use-data-lake-store.md:## Configure Data Lake store access
hdinsight-hadoop-use-data-lake-store.md:To configure Data Lake store access from your HDInsight cluster, you must have an Azure Active directory (Azure AD) service principal. Only an Azure AD administrator can create a service principal. The service principal must be created with a certificate. For more information, see [Configure Data Lake Store access](../data-lake-store/data-lake-store-hdinsight-hadoop-use-portal.md#configure-data-lake-store-access), and [Create service principal with self-signed-certificate](../azure-resource-manager/resource-group-authenticate-service-principal.md#create-service-principal-with-self-signed-certificate).
hdinsight-hadoop-use-data-lake-store.md:> If you are going to use Azure Data Lake Store as additional storage for HDInsight cluster, we strongly recommend that you do this while you create the cluster as described in this article. Adding Azure Data Lake Store as additional storage to an existing HDInsight cluster is a complicated process and prone to errors.
hdinsight-hadoop-use-data-lake-store.md:There are several ways you can access the files in Data Lake Store from an HDInsight cluster.
hdinsight-hadoop-use-data-lake-store.md:## Create HDInsight clusters with access to Data Lake Store
hdinsight-hadoop-use-data-lake-store.md:Use the following links for detailed instructions on how to create HDInsight clusters with access to Data Lake Store.
hdinsight-hadoop-use-data-lake-store.md:* [Using PowerShell (with Data Lake Store as default storage)](../data-lake-store/data-lake-store-hdinsight-hadoop-use-powershell-for-default-storage.md)
hdinsight-hadoop-use-data-lake-store.md:* [Using PowerShell (with Data Lake Store as additional storage)](../data-lake-store/data-lake-store-hdinsight-hadoop-use-powershell.md)
hdinsight-hadoop-use-data-lake-store.md:In this article, you learned how to use HDFS-compatible Azure Data Lake Store with HDInsight. This allows you to build scalable, long-term, archiving data acquisition solutions and use HDInsight to unlock the information inside the stored structured and unstructured data.
hdinsight-hadoop-use-data-lake-store.md:* [Get started with Azure Data Lake Store](../data-lake-store/data-lake-store-get-started-portal.md)
hdinsight-hadoop-use-data-lake-store.md:* [Create an HDInsight cluster to use Data Lake Store using the Azure portal](../data-lake-store/data-lake-store-hdinsight-hadoop-use-portal.md)
hdinsight-hadoop-use-data-lake-store.md:* [Create an HDInsight cluster to use Data Lake Store using the Azure PowerShell](../data-lake-store/data-lake-store-hdinsight-hadoop-use-powershell.md)
hdinsight-hadoop-windows-tools.md:## Data Lake (Hadoop) Tools for Visual Studio
hdinsight-hadoop-windows-tools.md:Use Data Lake Tools for Visual Studio to deploy and manage Storm topologies. Data Lake Tools also installs the SCP.NET SDK, which allows you to develop C# Storm topologies with Visual Studio.
hdinsight-hadoop-windows-tools.md:Before you go to the following examples, [install and try Data Lake Tools for Visual Studio](hadoop/apache-hadoop-visual-studio-tools-get-started.md). 
hdinsight-hadoop-windows-tools.md:Examples of tasks you can do with Visual Studio and Data Lake Tools for Visual Studio:
hdinsight-high-availability-linux.md:To ensure high availability of Hadoop services, HDInsight provides two head nodes. Both head nodes are active and running within the HDInsight cluster simultaneously. Some services, such as HDFS or YARN, are only 'active' on one head node at any given time. Other services such as HiveServer2 or Hive MetaStore are active on both head nodes at the same time.
hdinsight-key-scenarios-to-monitor.md:If your cluster's backing store is Azure Data Lake Store (ADLS), your throttling is most likely due to bandwidth limits. Throttling in this case could be identified by observing throttling errors in task logs. For ADLS, see the throttling section for the appropriate service in these articles:
hdinsight-key-scenarios-to-monitor.md:* [Performance tuning guidance for Hive on HDInsight and Azure Data Lake Store](../data-lake-store/data-lake-store-performance-tuning-hive.md)
hdinsight-key-scenarios-to-monitor.md:* [Performance tuning guidance for MapReduce on HDInsight and Azure Data Lake Store](../data-lake-store/data-lake-store-performance-tuning-mapreduce.md)
hdinsight-key-scenarios-to-monitor.md:* [Performance tuning guidance for Storm on HDInsight and Azure Data Lake Store](../data-lake-store/data-lake-store-performance-tuning-storm.md)
hdinsight-multiple-clusters-data-lake-store.md:title: Use multiple HDInsight clusters with an Azure Data Lake Store account - Azure | Microsoft Docs
hdinsight-multiple-clusters-data-lake-store.md:description: Learn how to use more than one HDInsight cluster with a single Data Lake Store account
hdinsight-multiple-clusters-data-lake-store.md:# Use multiple HDInsight clusters with an Azure Data Lake Store account
hdinsight-multiple-clusters-data-lake-store.md:Starting with HDInsight version 3.5, you can create HDInsight clusters with  Azure Data Lake Store accounts as the default filesystem.
hdinsight-multiple-clusters-data-lake-store.md:Data Lake Store supports unlimited storage that makes it ideal not only for hosting large amounts of data; but also for hosting multiple HDInsight clusters that share a single Data Lake Store Account. For instructions on how to create an HDInsight cluster with Data Lake Store as the storage, see [Create HDInsight clusters with Data Lake Store](../data-lake-store/data-lake-store-hdinsight-hadoop-use-portal.md).
hdinsight-multiple-clusters-data-lake-store.md:This article provides recommendations to the Data Lake store administrator for setting up a single and shared Data Lake Store Account that can be used across multiple **active** HDInsight clusters. These recommendations apply to hosting multiple secure as well as non-secure Hadoop clusters on a shared Data Lake store account.
hdinsight-multiple-clusters-data-lake-store.md:## Data Lake Store file and folder level ACLs
hdinsight-multiple-clusters-data-lake-store.md:The rest of this article assumes that you have a good knowledge of file and folder level ACLs in Azure Data Lake Store, which is described in detail at [Access control in Azure Data Lake Store](../data-lake-store/data-lake-store-access-control.md).
hdinsight-multiple-clusters-data-lake-store.md:## Data Lake Store setup for multiple HDInsight clusters
hdinsight-multiple-clusters-data-lake-store.md:Let us take a two-level folder hierarchy to explain the recommendations for using mutilple HDInsight clusters with a Data Lake Store account. Consider you have a Data Lake Store account with the folder structure **/clusters/finance**. With this structure, all the clusters required by the Finance organization can use /clusters/finance as the storage location. In the future, if another organization, say Marketing, wants to create HDInsight clusters using the same Data Lake Store account, they could create /clusters/marketing. For now, let's just use **/clusters/finance**.
hdinsight-multiple-clusters-data-lake-store.md:To enable this folder structure to be effectively used by HDInsight clusters, the Data Lake Store administrator must assign appropriate permissions, as described in the table. The permissions shown in the table correspond to Access-ACLs, and not Default-ACLs. 
hdinsight-multiple-clusters-data-lake-store.md:- **admin** is the creator and administrator of the Data Lake Store account.
hdinsight-multiple-clusters-data-lake-store.md:- The two level folder structure (**/clusters/finance/**) must be created and provisioned with appropriate permissions by the Data Lake Store admin **before** using the storage account for clusters. This structure is not created automatically while creating clusters.
hdinsight-multiple-clusters-data-lake-store.md:The limit on the number of clusters that can share a single Data Lake Store account depends on the workload being run on those clusters. Having too many clusters or very heavy workloads on the clusters that share a storage account might cause the storage account ingress/egress to get throttled.
hdinsight-multiple-clusters-data-lake-store.md:This section lists the known issues for using HDInsight with Data Lake Store, and their workarounds.
hdinsight-multiple-clusters-data-lake-store.md:When a new Azure Data Lake store account is created, the root directory is automatically provisioned with Access-ACL permission bits set to 770. The root folder’s owning user is set to the user that created the account (the Data Lake Store admin) and the owning group is set to the primary group of the user that created the account. No access is provided for "others".
hdinsight-multiple-clusters-data-lake-store.md:As stated in the YARN JIRA linked earlier, while localizing public resources, the localizer validates that all the requested resources are indeed public by checking their permissions on the remote file-system. Any LocalResource that does not fit that condition is rejected for localization. The check for permissions, includes read-access to the file for "others". This scenario does not work out-of-the-box when hosting HDInsight clusters on Azure Data Lake, since Azure Data Lake denies all access to "others" at root folder level.
hdinsight-multiple-clusters-data-lake-store.md:* [Create an HDInsight cluster with Data Lake Store as storage](../data-lake-store/data-lake-store-hdinsight-hadoop-use-portal.md)
hdinsight-release-notes.md:- **Support for Azure Data Lake Storage Gen2** – HDInsight will support the Preview release of Azure Data Lake Storage Gen2. In the available regions, customers will be able to choose an ADLS Gen2 account as a store for their HDInsight clusters.
hdinsight-release-notes.md:* Azure Data Lake Store access from Spark 2.1 clusters is not supported in this Preview.
hdinsight-run-samples.md://Store job configuration.
hdinsight-scaling-best-practices.md:> The `-D` switch is necessary because the default file system in HDInsight is either Azure Storage or Azure Data Lake Store. `-D` specifies that the commands execute against the local HDFS file system.
hdinsight-storage-sharedaccesssignature-permissions.md:* Stored access policy: A stored access policy is defined on a resource container, such as a blob container. A policy can be used to manage constraints for one or more shared access signatures. When you associate a SAS with a stored access policy, the SAS inherits the constraints - the start time, expiry time, and permissions - defined for the stored access policy.
hdinsight-streaming-at-scale-overview.md:In a streaming application, one or more data sources are generating events (sometimes in the millions per second) that need to be ingested  quickly  without dropping any useful information. The incoming events are handled with *stream buffering*, also called *event queuing*, by a service such as [Kafka](kafka/apache-kafka-introduction.md) or [Event Hubs](https://azure.microsoft.com/services/event-hubs/). After you collect the events, you can then analyze the data using a real-time analytics system within the *stream processing* layer, such as [Storm](storm/apache-storm-overview.md) or [Spark Streaming](spark/apache-spark-streaming-overview.md). The processed data can be stored in long-term storage systems, like [Azure Data Lake Store](https://azure.microsoft.com/services/data-lake-store/), and displayed in real time on a business intelligence dashboard, such as [Power BI](https://powerbi.microsoft.com), Tableau, or a custom web page.
hdinsight-streaming-at-scale-overview.md:Although you can specify the number of nodes in your cluster during creation, you may want to grow or shrink the cluster to match the workload. All HDInsight clusters allow you to [change the number of nodes in the cluster](hdinsight-administer-use-management-portal.md#scale-clusters). Spark clusters can be dropped with no loss of data, as all  data is stored in Azure Storage or Data Lake Store.
hdinsight-troubleshoot-failed-cluster.md:HDInsight relies on several Azure services. It runs virtual servers on Azure HDInsight, stores data and scripts on Azure Blob storage or Azure DataLake Store, and indexes log files in Azure Table storage. Disruptions to these services, although rare, can cause issues in HDInsight. If you have unexpected slowdowns or failures in your cluster,  check the [Azure Status Dashboard](https://azure.microsoft.com/status/). The status of each service is listed by region. Check your cluster's region and also regions for any related services.
hdinsight-troubleshoot-hdfs.md:Access the local HDFS from the command line and application code instead of by using Azure Blob storage or Azure Data Lake Store from inside the HDInsight cluster.   
hdinsight-upload-data.md:Azure HDInsight provides a full-featured Hadoop distributed file system (HDFS) over Azure Storage and Azure Data Lake Store. Azure Storage and Data lake Store are designed as an HDFS extension to provide a seamless experience to customers. They enable the full set of components in the Hadoop ecosystem to operate directly on the data it manages. Azure Storage and Data Lake Store are distinct file systems that are optimized for storage of data and computations on that data. For information about the benefits of using Azure Storage, see [Use Azure Storage with HDInsight][hdinsight-storage] and [Use Data Lake Store with HDInsight](hdinsight-hadoop-use-data-lake-store.md).
hdinsight-upload-data.md:    - [Use Data Lake Store with HDInsight](hdinsight-hadoop-use-data-lake-store.md)
hdinsight-use-external-metadata-stores.md:![HDInsight Hive Metadata Store Architecture](./media/hdinsight-use-external-metadata-stores/metadata-store-architecture.png)
hdinsight-use-external-metadata-stores.md:![HDInsight Hive Metadata Store Use Case](./media/hdinsight-use-external-metadata-stores/metadata-store-use-case.png)
hdinsight-use-external-metadata-stores.md:![HDInsight Hive Metadata Store Azure portal](./media/hdinsight-use-external-metadata-stores/metadata-store-azure-portal.png)
hdinsight-use-external-metadata-stores.md:![HDInsight Hive Metadata Store Ambari](./media/hdinsight-use-external-metadata-stores/metadata-store-ambari.png)
hdinsight-use-oozie-linux-mac.md:    > If the HDInsight cluster uses Azure Storage as the default storage, the `<value>` element contents begin with `wasb://`. If Azure Data Lake Store is used instead, it begins with `adl://`.
