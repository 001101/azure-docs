---
title: Conversation Transcription - Speech Service
titleSuffix: Azure Cognitive Services
description: "Conversation Transcription is a speech-to-text solution that combines speech recognition, speaker identification, and sentence attribution to each speaker (also known as diarization) to provide real-time and/or offline transcription of any conversation. This service makes conversations more inclusive for everyone, such as participants who are deaf and hard of hearing."
services: cognitive-services
author: markamos
manager: nitinme
ms.service: cognitive-services
ms.subservice: speech-service
ms.topic: conceptual
ms.date: 10/17/2019
ms.author: erhopf
---

# What is Conversation Transcription?

Conversation Transcription is a [speech-to-text](speech-to-text.md) solution that combines speech recognition, speaker identification, and sentence attribution to each speaker (also known as _diarization_) to provide real-time and/or offline transcription of any conversation. Conversation Transcription distinguishes speakers in a conversation to determine who said what and when, and makes it easy for developers to add speech-to-text to their applications that perform multi-speaker diarization.

## Key features

- **Timestamps** - each speaker utterance has a timestamp, so that you can easily find when a phrase was said.
- **Readable transcripts** - transcripts have formatting and punctuation added automatically to ensure the text closely matches what was being said.
- **User profiles** - user profiles are generated by collecting user voice samples and sending them to signature generation.
- **Speaker identification** - speakers are identified using user profiles and a _speaker ID_ is assigned to each.
- **Multi-speaker diarization** - determine who said what by synthesizing the audio stream with each speaker ID.
- **Real-time transcription** – provide live transcripts of who is saying what and when while the conversation is happening.
- **Offline transcription** – provide transcripts with higher accuracy by using a multichannel audio stream.

Although Conversation Transcription does not put a limit on the number of speakers in the room, it is optimized for 2-10 speakers per session.

## Use cases

### More inclusive meetings

To make meetings more inclusive for everyone, such as participants who are deaf and hard of hearing, it is imperative to have transcription in real time. Conversation Transcription in real-time mode takes meeting audio and determines who is saying what, allowing all meeting participants to follow the transcript and participate in the meeting without a delay.

### Improved meeting efficiency

Meeting participants can focus on the meeting and leave note-taking to Conversation Transcription. Participants can actively engage in the meeting and quickly follow up on next steps, using the transcript instead of taking notes and potentially missing something during the meeting.

## How it works

Below is a high-level overview diagram of how Conversation Transcription works.

![The Import Conversation Transcription Diagram](media/scenarios/conversation-transcription-service.png)

To build an end-to-end transcription solution, Conversation Transcription can be directly accessed through the Speech SDK. For step by step instructions, see [Transcribe multi-participant conversations with the Speech SDK](how-to-use-conversation-transcription-service.md).

Conversation Transcription expects the following inputs:

- **Multi-channel audio stream** – For specification and design details, see [Microsoft Speech Device SDK Microphone](https://aka.ms/cts/microphone). To learn more or purchase a development kit, see [Get Microsoft Speech Device SDK](https://aka.ms/cts/getsdk).
- **User voice samples** – Conversation Transcription needs user profiles in advance of the conversation. You will need to collect audio recordings from each user, then send the recordings to the [Signature Generation Service](https://aka.ms/cts/signaturegenservice) to validate the audio and generate user profiles.

## Real-time vs. offline transcription modes

Conversation Transcription offers three transcription modes:

### Real-time

Audio data is processed live to return speaker ID + transcript. Select this mode if your transcription solution requirement is to provide conversation participants a live transcript view of their ongoing conversation. For example, building an application to make meetings more accessible the deaf and hard of hearing participants is an ideal use case for real-time transcription.

### Offline

Audio data is batch processed to return speaker ID + transcript. Select this mode if your transcription solution requirement is to provide higher accuracy without live transcript view. For example, if you want to build an application to allow meeting participants to easily catch up on missed meetings, then use the offline transcription mode to get high-accuracy transcription results.

### Real-time plus Offline

Audio data is processed live to return speaker ID + transcript, and, in addition, a request is created to also get a high-accuracy transcript through offline processing. Select this mode if your application has a need for real-time transcription but also requires a higher accuracy transcript for use after the conversation or meeting occurred.

## Language support

Currently, Conversation Transcription supports "en-US" and "zh-CN" in the following regions: *centralus* and *eastasia*. If you require additional locale support, contact the [Conversation Transcription Feature Crew](mailto:CTSFeatureCrew@microsoft.com).

## Next steps

> [!div class="nextstepaction"][transcribe multi-participant conversations with the speech sdk](how-to-use-conversation-transcription-service.md)
