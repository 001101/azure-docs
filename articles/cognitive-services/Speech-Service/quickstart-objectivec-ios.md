---
title: 'Quickstart: Recognize speech in Objective-C on iOS using the Cognitive Services Speech SDK'
titleSuffix: "Microsoft Cognitive Services"
description: Learn how to recognize speech in Objective-C on iOS using the Cognitive Services Speech SDK
services: cognitive-services
author: chlandsi

ms.service: cognitive-services
ms.technology: Speech
ms.topic: article
ms.date: 09/24/2018
ms.author: chlandsi
---

# Quickstart: Recognize speech in Objective-C on iOS using the Cognitive Services Speech SDK

[!INCLUDE [Selector](../../../includes/cognitive-services-speech-service-quickstart-selector.md)]

In this article, you learn how to create an iOS app in Objective-C using the Cognitive Services Speech SDK to transcribe an audio file with recorded speech to text.

## Prerequisites

* A subscription key for the Speech service. See [Try the speech service for free](get-started.md).
* A Mac with Xcode 9.4.1 installed as iOS development environment. This tutorial targets iOS versions 11.4. If you don't have Xcode yet, you can install it from the [App Store](https://geo.itunes.apple.com/us/app/xcode/id497799835?mt=12).

## Get the Speech SDK for iOS

[!INCLUDE [License Notice](../../../includes/cognitive-services-speech-service-license-notice.md)]

The current version of the Cognitive Services Speech SDK is `1.0.0`.

The Cognitive Services Speech SDK for Mac and iOS can be downloaded as a zip-file from https://aka.ms/csspeech/iosbinary. Download and copy the files to the `speechsdk` subdirectory in your home directory.


## Create an Xcode Project 

Start Xcode, and start a new project by clicking **File** > **New** > **Project**.
In the template selection dialog, choose the "iOS Single View App" template.

In the dialogs that follow, make the following selections:

1. Project Options Dialog
    1. Enter a name for the quickstart app, for example `helloworld`.
    1. Enter an organization name such as `TestOrg`, and an organization identifer such as `testorg`.
    1. Make sure Objective-C is chosen as the language for the project.
    1. Disable all checkboxes for tests and core data.
    ![Project Settings](media/sdk/qs-objectivec-project-settings.png)
1. Select project directory
    1. Choose your home directory to put the project in. This will create a `helloworld` directory in your home directory that contains all the files for the Xcode project.
    1. Disable the creation of a Git repo for this example project.
    1. Adjust the paths to the SDK in the *Project Settings*.
        1. In the **General** tab under the **Linked Frameworks and Libraries** header, add the SDK library as a framework: **Add framework** > **Add other...** > Navigate to `speechsdk/lib/` in your home directory and choose the file `libMicrosoft.CognitiveServices.Speech.core.dylib`.
        1. Go to the **Build Settings** tab and activate **All** settings.
        1. Add the path to SDK headers (`$(SRCROOT)/../speechsdk/include`) to the *Header Search Path* under the **Search Paths** heading.
        ![Header Search Path setting](media/sdk/qs-objectivec-header-search-path.png)
        1. Add the path to the directory containing the library (`$(SRCROOT)/../speechsdk/lib`) to the *Library Search Path* under the **Search Paths** heading.
        ![Library Search Path setting](media/sdk/qs-objectivec-library-search-path.png)
        1. Add the path to the directory containing the library (`$(SRCROOT)/../speechsdk/lib`) to the *Runpath Search Path* under the **Linking** heading.
        ![Runpath Search Path setting](media/sdk/qs-objectivec-runpath-search-path.png)

## Set up the UI

The example app will have a very simple UI: A button to start the processing of the file, and a text label to display the result.
The UI is set up in the `Main.storyboard` part of the project.
Open the XML view of the storyboard by right-clicking the `Main.storyboard` entry of the project tree and selecting **Open As...** > **Source Code**.
Replace the autogenerated XML with this:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<document type="com.apple.InterfaceBuilder3.CocoaTouch.Storyboard.XIB" version="3.0" toolsVersion="14113" targetRuntime="iOS.CocoaTouch" propertyAccessControl="none" useAutolayout="YES" useTraitCollections="YES" useSafeAreas="YES" colorMatched="YES" initialViewController="BYZ-38-t0r">
    <device id="retina4_7" orientation="portrait">
        <adaptation id="fullscreen"/>
    </device>
    <dependencies>
        <deployment identifier="iOS"/>
        <plugIn identifier="com.apple.InterfaceBuilder.IBCocoaTouchPlugin" version="14088"/>
        <capability name="Safe area layout guides" minToolsVersion="9.0"/>
        <capability name="documents saved in the Xcode 8 format" minToolsVersion="8.0"/>
    </dependencies>
    <scenes>
        <!--View Controller-->
        <scene sceneID="tne-QT-ifu">
            <objects>
                <viewController id="BYZ-38-t0r" customClass="ViewController" sceneMemberID="viewController">
                    <view key="view" contentMode="scaleToFill" id="8bC-Xf-vdC">
                        <rect key="frame" x="0.0" y="0.0" width="375" height="667"/>
                        <autoresizingMask key="autoresizingMask" widthSizable="YES" heightSizable="YES"/>
                        <subviews>
                            <button opaque="NO" contentMode="scaleToFill" fixedFrame="YES" contentHorizontalAlignment="center" contentVerticalAlignment="center" buttonType="roundedRect" lineBreakMode="middleTruncation" translatesAutoresizingMaskIntoConstraints="NO" id="qFP-u7-47Q">
                                <rect key="frame" x="99" y="247" width="176" height="82"/>
                                <autoresizingMask key="autoresizingMask" flexibleMaxX="YES" flexibleMaxY="YES"/>
                                <fontDescription key="fontDescription" type="system" pointSize="30"/>
                                <state key="normal" title="Recognize!"/>
                                <connections>
                                    <action selector="recognizeButtonTapped:" destination="BYZ-38-t0r" eventType="touchUpInside" id="Vfr-ah-nbC"/>
                                </connections>
                            </button>
                            <label opaque="NO" userInteractionEnabled="NO" contentMode="center" horizontalHuggingPriority="251" verticalHuggingPriority="251" fixedFrame="YES" text="Recognition result" textAlignment="center" lineBreakMode="tailTruncation" numberOfLines="5" baselineAdjustment="alignBaselines" adjustsFontSizeToFit="NO" translatesAutoresizingMaskIntoConstraints="NO" id="tq3-GD-ljB">
                                <rect key="frame" x="20" y="408" width="335" height="148"/>
                                <autoresizingMask key="autoresizingMask" flexibleMaxX="YES" flexibleMaxY="YES"/>
                                <fontDescription key="fontDescription" type="system" pointSize="30"/>
                                <nil key="textColor"/>
                                <nil key="highlightedColor"/>
                            </label>
                        </subviews>
                        <color key="backgroundColor" red="1" green="1" blue="1" alpha="1" colorSpace="custom" customColorSpace="sRGB"/>
                        <viewLayoutGuide key="safeArea" id="6Tk-OE-BBY"/>
                    </view>
                    <connections>
                        <outlet property="recognitionResultLabel" destination="tq3-GD-ljB" id="kP4-o4-s0Q"/>
                        <outlet property="recognizeButton" destination="qFP-u7-47Q" id="dqX-Pp-pCL"/>
                    </connections>
                </viewController>
                <placeholder placeholderIdentifier="IBFirstResponder" id="dkx-z0-nzr" sceneMemberID="firstResponder"/>
            </objects>
            <point key="canvasLocation" x="135.19999999999999" y="132.68365817091455"/>
        </scene>
    </scenes>
</document>

```

## Add the sample code

1. Download the [sample wav file](https://raw.githubusercontent.com/Azure-Samples/cognitive-services-speech-sdk/f9807b1079f3a85f07cbb6d762c6b5449d536027/samples/cpp/windows/console/samples/whatstheweatherlike.wav) by right-clicking the link and choosing **Save target as...**.
Add the wav file to the project as a resource by dragging it from a Finder window into the root level of the Project view.
Click **Finish** in the following dialog without changing the settings.
1. Replace the contents of the autogenerated `ViewController.m` file by:
    ```objectivec
    #import "ViewController.h"
    #import "speech_factory.h"

    @interface ViewController ()
    @property (weak, nonatomic) IBOutlet UIButton *recognizeButton;
    @property (weak, nonatomic) IBOutlet UILabel *recognitionResultLabel;
    - (IBAction)recognizeButtonTapped:(UIButton *)sender;
    @end

    @implementation ViewController

    - (void)viewDidLoad {
        [super viewDidLoad];
        // Do any additional setup after loading the view.
    }

    - (void)didReceiveMemoryWarning {
        [super didReceiveMemoryWarning];
        // Dispose of any resources that can be recreated.
    }

    - (IBAction)recognizeButtonTapped:(UIButton *)sender {
        __block bool end = false;
        SpeechFactory *factory = [SpeechFactory fromSubscription:@"YourSubscriptionKey" AndRegion:@"YourServiceRegion"];

        NSBundle *mainBundle = [NSBundle mainBundle];
        NSString *myFile = [mainBundle pathForResource: @"whatstheweatherlike" ofType: @"wav"];
        NSLog(@"Main bundle path: %@", mainBundle);
        NSLog(@"myFile path: %@", myFile);

        SpeechRecognizer *recognizer = [factory createSpeechRecognizerWithFileInput:myFile];

        [recognizer addFinalResultEventListener: ^ (SpeechRecognizer * recognizer, SpeechRecognitionResultEventArgs *eventArgs) {
            NSLog(@"Received final result event. SessionId: %@, recognition result:%@. Status %ld.", eventArgs.sessionId, eventArgs.result.text, (long)eventArgs.result.recognitionStatus);
            dispatch_async(dispatch_get_main_queue(), ^{
                [self updateRecognitionResultText:(eventArgs.result.text)];
            });
            end = true;
        }];

        [recognizer startContinuousRecognition];
        while (end == false)
            [NSThread sleepForTimeInterval:1.0f];
        [recognizer stopContinuousRecognition];

        [recognizer close];
    }

    - (void)updateRecognitionResultText:(NSString *) resultText {
        self.recognitionResultLabel.text = resultText;
    }

    @end
    ```
    <!-- [!code-cpp[Quickstart Code](~/samples-cognitive-services-speech-sdk/quickstart/objectivec-ios/helloworld/helloworld/ViewController.m#code)] -->
1. Replace the string `YourSubscriptionKey` with your subscription key.

1. Replace the string `YourServiceRegion` with the [region](regions.md) associated with your subscription (for example, `westus` for the free trial subscription).


## Building and Running the Sample

1. Make the debug output visible (**View** > **Debug Area** > **Activate Console**).
1. Build and run the example code in the iOS simulator by selecting **Product** -> **Run** from the menu or clicking the **Play** button.
1. After you click the "Recognize!" button in the app, you should see the contents of the audio file "What's the weather like?" on the lower part of the simulated screen.

 ![Simulated iOS App](media/sdk/qs-objectivec-simulated-app.png)

[!INCLUDE [Download the sample](../../../includes/cognitive-services-speech-service-speech-sdk-sample-download-h2.md)]
Look for this sample in the `quickstart/objectivec-ios` folder.

## Next steps

* [Get our samples](speech-sdk.md#get-the-samples)
