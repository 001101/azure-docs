
# Comparing Azure Data Lake Storage (Preview) and Azure Data Lake Store

## Comparing Azure Data Lake Storage (Preview) and Azure Data Lake Store

The table in this article summarizes the differences between Azure Data Lake Store and Azure Data Lake Storage (Preview).

Azure Data Lake Store is a hyper scale repository that is optimized for big data analytics workloads.

In comparison, Azure Data Lake Storage (Preview) offers rich support for the Hadoop Compatible File System along with a full hierarchical namespace optimized for analytics workloads. Additionally it brings all of Azure Blobs offerings to the table so functionality like data tiering and lifecycle management, durability options for HA and DR, eventgrid support, enhanced network security, and compatibility with all of Blob APIs.

|Azure Data Lake Store  |Azure Data Lake Storage (Preview)  |
|---------|---------|
|Compatible to HDFS standard – customer can perform analysis using the Hadoop analytic framework.  Applications or services that use the Hadoop file system API can easily integrate with ADLS.  ADLS exposes a HDFS-compatible REST interface for applications.     |Hadoop File System Compatible – customers can use Hadoop analytics frameworks that leverage the Hadoop FileSystem API. WASB & Prague is part of the Apache Hadoop project.          |
|HDFS extensions – customers can use non-destructive concat, set expiry, acquire leases on files being written, etc.       |         |
|Hierarchical file system – customer can store files into a true hierarchical folder structure.     |Hierarchical file system – customer can store files into a true hierarchical folder structure.         |
|Atomic renames/moves – customer can rename or move folders and files atomically.     |Atomic renames/moves – customer can rename or move folders and files atomically.           |
|File system consistency - customer can ensure that their data is consistent.  Data is readable immediately after it is written.     |File system consistency - customer can ensure that their data is consistent.  Data is readable immediately after it is written.         |
|Partition management handled  – customer can set up their file architecture logically.  Their data is partitioned appropriately so that partitions are not throttled or have reduced performance when they become hot.  This avoids customers having to re-architect their directory structure or having unnatural directory structures to get scalable performance.     |         |
|No fixed throughput/IOPS limit – customer can have massive throughput to support any size of analytics.      |Throughput/IOPS limit – customer can have massive throughput to support any size of analytics.         |
|Massively parallel read and writes – customer can spread data across many servers  to enable large amounts of data to be read or written in parallel.  Customer can perform an unlimited number of IOPs per file.     |Massively parallel read and writes – customer can spread data across many servers to enable large amounts of data to be read or written in parallel.         |
|Unlimited storage – customer can store an unlimited amount of data for an unlimited amount of time.  There are no limits to accounts sizes, files sizes, and number of files.     |Storage capacity – customer can store a massive amount of data for an unlimited amount of time.           |
|Petabyte-sized files – customer can store large petabyte-sized files.  There is no limit on file sizes.     |         |
|Unlimited appends per file – customer can read and write to unlimited append blocks in parallel.     |         |
|Millions of files per folder - customer can have millions of files in a single folder at any level in the folder hierarchy     |         |
|"Unlimited throughput per file - customer can use all the throughput for the account to read or write to a specific file"     |         |
|First-class integration with AAD – customer can use AAD features including multi-factor authentication, conditional access, role-based access control, user lifecycle management, application usage monitoring, security monitoring, and alerting.     |First-class integration with AAD – customer can use AAD features including multi-factor authentication, conditional access, role-based access control, user lifecycle management, application usage monitoring, security monitoring, and alerting.         |
|POSIX compliant Access Control Lists – customer can have fine grained access control on the file and folder level.  Customer can easily understand access control because it is defined using the POSIX access control model.     |POSIX compliant Access Control Lists – customer can have fine grained access control on the file and folder level.  Customer can easily understand access control because it is defined using the POSIX access control model.         |
|Encryption – customer can have encryption enabled by default.  Customer data is encrypted at rest, movement within ADLS, and over the wire when communicating outside ADLS.     |Encryption – customer can have encryption enabled by default.  Customer data is encrypted at rest, movement within ADLS, and over the wire when communicating outside Prague.         |
|Allow secure communication only via HTTPS – customer can have secure communication into ADLS because all data ingested into ADLS must be encrypted.     |Allow secure communication only via HTTPS – customer can have secure communication into Prague because all data ingested into Prague must be encrypted.         |
|Key management – customer can have keys managed by Azure or customer-managed keys via Key Vault.     |Key management – customer can have keys managed by Azure or customer-managed keys via Key Vault.         |
|VNET support – customer can allow access to ADLS from a specific VNET to restrict access.     |VNET support – customer can allow access to Prague from a specific VNET and/or IP addresses to restrict access.         |
|High availability – customer can have their data available at least 99.9% of the time.     |High availability – customer can have their data available at least 99.99% of the time.         |
|Regional availability – customer can use ADLS in East US 2, Central US, North Europe, West Europe     |Regional availability – customer can use Prague in all Azure public and private regions.         |
|First party integration – customer can have integration with first party applications (e.g. HDInsight, Azure VMs, ASA, Event Hubs, ADF, Powershell, ExpressRoute, Azure Import/Export, Azure CLI, R, SQL DW via Polybase, AAD, Key Vault, Data Catalog, PowerBI, Analysis Services, Visual Studio).     |First party integration – customer can have integration with first party applications (e.g. HDInsight, Azure Databricks, Azure VMs, ASA, Event Hubs, ADF, Powershell, ExpressRoute, Azure Import/Export, Azure Databox, Azure CLI, R, SQL DW via Polybase, AAD, Key Vault, Data Catalog, PowerBI, Azure Analysis Services, EventGrid, Visual Studio).         |
|3rd party integration – customer can have integration with 3rd party applications (e.g. Cloudera, Hortonworks, Nifi, Qubole, Informatica, StreamSets, ImanisData, Paxata, Trifacta).     |3rd party integration – customer can have integration with 3rd party applications (e.g. Hortonworks, MapR, Nifi, Qubole, Informatica, StreamSets, ImanisData, Paxata, Trifacta).         |
|Open Source Software – customer can use Spark, Hive, Tez, MapReduce, Storm, HBase, Sqoop, HCatalog, Mahout, Pig, Pig Latin, Oozie, Zookeeper.     |Open Source Software – customer can use Spark, Hive, Tez, MapReduce, Storm, HBase, Sqoop, HCatalog, Mahout, Pig, Pig Latin, Oozie, Zookeeper.         |
|Data lifecycle policy: File expiration – customer can delete files at a specified date and time to reduce cost and manage compliance.     |Lifecycle Management Policies – customers will be able to define policies to control the automatic movement of data across the multiple tiers         |
|Auditing – customer can have request and diagnostic logs collect data to access audit trails.     |         |
|APIs – customer can use REST APIs over HTTPS.     |APIs – customer can use REST APIs over HTTP/HTTPS.         |
|Many ways to read and write data – customer can use SDKs (.NET, Java, Python, R), Azure portal, and tools (Powershell, Azure CLI, AdlCopy, Distcp, ADF, Azure Import/Export) to read and write their data.     |Many ways to read and write data – Azure portal, and tools (Azure Storage Explorer, Powershell, Azure CLI, AzCopy, Distcp, ADF, Azure Import/Export, Azure Databox) to read and write their data.         |


To Do: Add a couple transition sentences to the migration article.